{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Beating Classifiers\n",
      "\n",
      "### Please Note\n",
      "\n",
      "I am not a spammer! I've studied machine learning and classification over the last year and became interested in the topic. I've always thought that a good way to learn about how something works is to break it. The desire to break classification systems in order to learn about them is what drove me to work on this project. \n",
      "\n",
      "## Introduction\n",
      "\n",
      "Imagine we are sending messages to a classifier for which we can only observe the input features and the classification result. For example: \n",
      "\n",
      "- Sending messages to an email inbox where they are filtered by a spam classifier. \n",
      "- Asking bots to scrape a website where they are filtered by a bot detection algorithm. \n",
      "\n",
      "In these cases we can have access to the data which is sent to the classifier and we can find some way of getting the classification result.\n",
      "\n",
      "Using this information, we want to devise an method that can beat the classifier. \"Beating the classifier\" means crafting feature vectors which:\n",
      "\n",
      "- Satisify some requirements we define, and \n",
      "- Are not rejected by the classifier\n",
      "\n",
      "## Preliminaries\n",
      "\n",
      "### Prerequisites\n",
      "\n",
      "- Familiarity with machine learning techniques and terminology, e.g. \"classifier\", \"training set\", etc. \n",
      "- Familiarity with the Logistical Regresssion Classifier specifically\n",
      "\n",
      "### Definitions\n",
      "\n",
      "- The Classifier: The classifier we are trying to beat\n",
      "- The Anti-Classifier: The algorithm we write to try and beat the classifier\n",
      "- Message: The raw message / object that we is constructed by the anti-classifier and sent to the classifier\n",
      "- Feature Vector: A vector of features extracted from the message\n",
      "- Feature Specificiations: The data type, max / min value and name of each variable entry in the feature vector\n",
      "- Feature Contraints: A set of requirements we want our feature vector to satisfy while still passing through the classifier\n",
      "\n",
      "## General Approach\n",
      "\n",
      "We tackle this problem by building our own anti-classifier which learns the behaviour of the target classifier. By manipulating the decision function of this anti-classifier we should be able craft feature vectors which pass through the classifier. \n",
      "\n",
      "We proceed as follows:\n",
      "\n",
      "- Stage One: Train the Anti-Classifier\n",
      "\n",
      "    1. Define feature specifications, i.e. name, max value, min value, data type for each feature used by the classifier\n",
      "    1. Randomly generate a set of intial feature vectors that conform to the specifications\n",
      "    1. Create a logisitical regression classifier to use as our anti-classifier. \n",
      "    1. Create messages out of the randomly generated set of feature vectors and send these messages to the classifier \n",
      "    1. Record the result for each message\n",
      "    1. Use the results to train the anti-classifier\n",
      "    \n",
      "At this point, the anti-classifier is trained on which messages pass through the classifier. \n",
      "\n",
      "- Stage Two: Use the Anti-Classifier\n",
      "    \n",
      "    1. Create feature constraints, i.e. a range of values that features must fall into\n",
      "    1. Generate a feature vector that minimizes the decision function of the anti-classifier under those contraints\n",
      "    1. Output the result\n",
      "\n",
      "The result is a feature vector which we expect to not be caught by the classifier. \n",
      "\n",
      "### Simplifications\n",
      "\n",
      "One of the most complex and error-prone portions of this approach would be a translation layer between feature vectors and messages (Stage 1, step 4, above). A feature vector would go through the following transformations:\n",
      "\n",
      "    Anti-Classifier Features --(Anti-Classifier Translation)--> Messages --(Classifier Translation)--> Classifier Features\n",
      "\n",
      "This presents us with several difficulties:\n",
      "\n",
      "- We are not allowed to know the Classifier Translation step\n",
      "- The Anti-Classifier translation layer is specific to each use case\n",
      "- We risk missing features that are significant to the classifier\n",
      "\n",
      "For the purposes of this document we ignore the translation layer and concentrate on the core algorithm. Do do this, we simply grant the the anti-classifier access to the feature specifications of the classifier. The message-passing therefore takes the form:\n",
      "\n",
      "    Anti-Classifier Features --> Classifier Features\n",
      "\n",
      "The features do not undergo any transformation between the two components. However we avoid sharing too much information by not giving the anti-classifier access to the classifier parameters or the classifier training set. \n",
      "\n",
      "### Test Data\n",
      "\n",
      "For this project we use the [Spambase Data Set](https://archive.ics.uci.edu/ml/datasets/Spambase) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.html). \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Code\n",
      "\n",
      "The code for the this algorithm can be found in the `src/` directory. \n",
      "\n",
      "### Initialization \n",
      "\n",
      "We begin by loading test data using `src/data.py`.  \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src import data\n",
      "xtrain, xtest, ytrain, ytest = data.load_spambase_test_train()\n",
      "print(\"Number of features: {0}\".format(len(xtrain.columns)))\n",
      "xtrain[xtrain.columns[:5]].describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of features: 57\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>capital_run_length_average</th>\n",
        "      <th>capital_run_length_longest</th>\n",
        "      <th>capital_run_length_total</th>\n",
        "      <th>char_freq_!</th>\n",
        "      <th>char_freq_#</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td>3082.000000</td>\n",
        "      <td>3082.000000</td>\n",
        "      <td>3082.000000</td>\n",
        "      <td>3082.000000</td>\n",
        "      <td>3082.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>4.899553</td>\n",
        "      <td>52.884491</td>\n",
        "      <td>289.825438</td>\n",
        "      <td>0.276092</td>\n",
        "      <td>0.050973</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>25.285658</td>\n",
        "      <td>216.191784</td>\n",
        "      <td>609.223593</td>\n",
        "      <td>0.906813</td>\n",
        "      <td>0.496842</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>1.576250</td>\n",
        "      <td>6.000000</td>\n",
        "      <td>34.000000</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>2.275500</td>\n",
        "      <td>14.000000</td>\n",
        "      <td>93.500000</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>3.706750</td>\n",
        "      <td>43.000000</td>\n",
        "      <td>267.000000</td>\n",
        "      <td>0.326000</td>\n",
        "      <td>0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>667.000000</td>\n",
        "      <td>9989.000000</td>\n",
        "      <td>10062.000000</td>\n",
        "      <td>32.478000</td>\n",
        "      <td>19.829000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "       capital_run_length_average  capital_run_length_longest  \\\n",
        "count                 3082.000000                 3082.000000   \n",
        "mean                     4.899553                   52.884491   \n",
        "std                     25.285658                  216.191784   \n",
        "min                      1.000000                    1.000000   \n",
        "25%                      1.576250                    6.000000   \n",
        "50%                      2.275500                   14.000000   \n",
        "75%                      3.706750                   43.000000   \n",
        "max                    667.000000                 9989.000000   \n",
        "\n",
        "       capital_run_length_total  char_freq_!  char_freq_#  \n",
        "count               3082.000000  3082.000000  3082.000000  \n",
        "mean                 289.825438     0.276092     0.050973  \n",
        "std                  609.223593     0.906813     0.496842  \n",
        "min                    1.000000     0.000000     0.000000  \n",
        "25%                   34.000000     0.000000     0.000000  \n",
        "50%                   93.500000     0.000000     0.000000  \n",
        "75%                  267.000000     0.326000     0.000000  \n",
        "max                10062.000000    32.478000    19.829000  "
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`xtrain` and `ytrain` are the feature vectors and classification labels for the training set. `xtest` and `ytest` make up the test set. There are 57 features in total. \n",
      "\n",
      "We can now initialize and train one of the classifiers from `src/classifier.py`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src import classifier\n",
      "logistic = classifier.logistic()\n",
      "logistic.fit(xtrain, ytrain)\n",
      "logistic.score(xtest, ytest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "0.92626728110599077"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The trained classfier has a precision of 93% on the test set. \n",
      "\n",
      "To create an anticlassifier we need the specifications which define each of the features in the vectors we will generate. The file `src/features.py` examines the spambase training set and generates these specifications for us:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pprint import pprint\n",
      "from src import features\n",
      "feature_specs = features.SPAMBASE_FEATURE_SPECS\n",
      "pprint(feature_specs[:5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'max': 667.0,\n",
        "  'min': 1.0,\n",
        "  'name': 'capital_run_length_average',\n",
        "  'type': <type 'float'>},\n",
        " {'max': 9989,\n",
        "  'min': 1,\n",
        "  'name': 'capital_run_length_longest',\n",
        "  'type': <type 'int'>},\n",
        " {'max': 10062,\n",
        "  'min': 1,\n",
        "  'name': 'capital_run_length_total',\n",
        "  'type': <type 'int'>},\n",
        " {'max': 32.478000000000002,\n",
        "  'min': 0.0,\n",
        "  'name': 'char_freq_!',\n",
        "  'type': <type 'float'>},\n",
        " {'max': 19.829000000000001,\n",
        "  'min': 0.0,\n",
        "  'name': 'char_freq_#',\n",
        "  'type': <type 'float'>}]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The feature specs define maximum values, minimum values, and data types for each feature. \n",
      "\n",
      "With this data in hand we can create the anti-classifier using the file `src/anticlassifier.py`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src import anticlassifier\n",
      "anti = anticlassifier.AntiClassifier(logistic, feature_specs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Upon initialization the `AntiClassifier` object randomly creates 10,000 feature vectors using the provided specifications. It sends these feature vectors to the classifier and records the result. It then uses this information to train it's own anti-classifier. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anti.anticlassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "Pipeline(steps=[('transform', Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True))])), ('logistic', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
        "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
        "          verbose=0))])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parameters of this anti-classifier can be examined:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anti._lg_coefs()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "array([-0.61873188,  5.13519473,  0.60365299,  0.72393365,  3.37719023,\n",
        "        2.43333112, -0.10897717, -0.52533086, -1.07981114,  0.98761438,\n",
        "       -0.38281584,  1.28346311, -1.18936015,  0.20036899, -2.21947156,\n",
        "        0.31192398, -0.14928574,  0.23866955,  0.02906303,  0.54005145,\n",
        "       -1.44143269,  1.2969284 , -2.70402877, -1.37071834, -0.26097976,\n",
        "       -1.92813128, -0.02310345,  0.46859701,  1.74747417, -3.18124512,\n",
        "       -2.53656871, -0.87622423,  0.47917143, -1.37873785, -0.85579594,\n",
        "        0.10282952, -0.24446081, -1.55228842,  0.27339238,  0.24794368,\n",
        "       -0.32876098,  0.33334886,  0.32205744, -0.44315146,  0.05625021,\n",
        "       -0.68385665, -1.94441933, -1.63857318, -0.05890673,  1.29866886,\n",
        "       -0.00535188, -0.49505018,  0.35278402,  0.00536339, -0.13407816,\n",
        "        0.18434259,  0.22940819])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Generating Feature Vectors\n",
      "\n",
      "The `get()` method is used to generate a feature vector which minimizes the anti-classifier's decision function under a set of provided constraints. We can generate such feature vector with no constraints:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = anti.get([])\n",
      "print(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  7.35439850e+01   8.50000000e+03   1.33200000e+03   2.80914816e+01\n",
        "   1.41215550e+01   1.23645779e+00   1.97845256e+00   3.29474874e+00\n",
        "   3.12445249e+00   1.12199727e+00   6.57448798e+00   9.02158310e+00\n",
        "   3.66680163e+00   1.07507762e-01   3.34570774e+00   2.38898675e+00\n",
        "   1.10913607e+01   2.60154551e+00   6.58395051e-01   1.82085907e+00\n",
        "   2.66126777e+00   4.16883498e+00   4.42286014e+00   6.43082750e+00\n",
        "   3.22378040e+00   1.20344431e+01   1.10647036e+00   6.67252920e+00\n",
        "   5.07259744e+00   2.62756395e+01   1.27373522e+01   5.58322512e+00\n",
        "   3.83759509e+00   3.10603317e+00   1.16214413e+00   6.09265147e+00\n",
        "   3.84765288e+00   8.16858206e+00   6.26210160e+00   2.00179615e-01\n",
        "   1.45672288e-01   1.39715421e+00   3.94100021e+00   6.56938904e+00\n",
        "   4.94357837e+00   6.24472913e+00   1.36274089e+01   1.70023371e+01\n",
        "   1.93711368e+00   6.34951617e-01   8.34409856e+00   3.85888515e-01\n",
        "   3.65409395e+00   2.34284863e+00   2.99762621e+00   1.33350085e+01\n",
        "   3.65493157e+00]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By passing this generated feature vector to the classifier we see that it passes through, as expected (a zero value indicates \"not-spam\" in the spambase data set). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logistic.predict(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "array([0])"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also provide a set of constraints to get a different feature vector. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint(features.SPAMBASE_CONSTRAINTS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'fun': <function <lambda> at 0x10e4e1320>,\n",
        "  'init': <function <lambda> at 0x10e4e1500>,\n",
        "  'name': 'capital_run_length_average_gt_10',\n",
        "  'type': 'ineq'},\n",
        " {'fun': <function <lambda> at 0x10e4e15f0>,\n",
        "  'init': <function <lambda> at 0x10e4e1668>,\n",
        "  'name': 'capital_run_length_longest_gt_5',\n",
        "  'type': 'ineq'}]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = anti.get(features.SPAMBASE_CONSTRAINTS)\n",
      "print(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  2.36194926e+03   1.94623933e+04   7.91007283e+03   1.61748113e+01\n",
        "   1.15304713e+01   2.99625281e+00   4.90278861e+00   2.98186954e+00\n",
        "   3.29083923e+00   2.72814438e+00   4.32565016e+00   2.12182965e+01\n",
        "   4.47366499e+00   6.32122023e+00   1.21362554e+01   2.36598323e+00\n",
        "   1.22545161e+01   2.36084374e+00   2.53931591e+00   5.18143105e+00\n",
        "   5.63996413e+00   9.08576980e+00   4.37514487e+00   1.03639840e+01\n",
        "   4.14656648e+00   1.40732651e+01   3.51354877e+00   8.49531770e+00\n",
        "   1.00115062e+01   2.61101913e+01   1.52323113e+01   4.63741300e+00\n",
        "   5.53442803e+00   6.35787683e+00   3.17865299e+00   1.11388304e+01\n",
        "   3.75215499e+00   6.45368264e+00   6.29659008e+00   1.67511228e+00\n",
        "   2.79363192e+00   9.27531995e+00   4.98628442e+00   5.80977582e+00\n",
        "   2.77686565e+00   5.94391629e+00   1.69205771e+01   2.07721614e+01\n",
        "   1.32951414e+00   6.79127854e+00   5.09277200e+00   1.13679431e+00\n",
        "   2.37525271e+00   2.65700027e+00   5.94190610e+00   1.00443275e+01\n",
        "   5.54243058e+00]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logistic.predict(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "array([0])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Testing the Anti-Classifier\n",
      "\n",
      "To test the performance of the anti-classifier we proceed as follows:\n",
      "\n",
      "- Determine the most significant features in classification\n",
      "- Constrain these features one by one and test how well the anti-classifier performs\n",
      "\n",
      "This process is implemented in the `src/evaluate.py` file. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src import evaluate"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most significant features are determined by examining the p-values of an ANOVA hypothesis test. These features are then constrained to their maximum values to determine how well the anti-classifier performs. \n",
      "\n",
      "We can perform this test for several different target classifiers. \n",
      "\n",
      "### Logistical Regression Classifier\n",
      "\n",
      "The logistical regression classifier was already initialized, above. To test it we pass it to the `evaluate.evaluate()` function. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "perf_max_constraint = evaluate.evaluate(logistic, constrain=\"max\")\n",
      "perf_max_constraint.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(perf_max_constraint[\"significant_features_constrained\"], perf_max_constraint[\"anticlassifier_score\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By maximizing just 3 of the 57 features used by the classifier we become completely unable to generate vectors which are accepted. Most machine-learning problem spaces have only a few features which dominate the output so this behaviour is expected. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The Naive Bayes Classifier\n",
      "\n",
      "Next we target a naive bayes classifier and perform the same test. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bayes = classifier.naive_bayes()\n",
      "bayes.fit(xtrain, ytrain)\n",
      "print(bayes.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "perf_max_constraint = evaluate.evaluate(bayes, constrain=\"max\")\n",
      "perf_max_constraint.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(perf_max_constraint[\"significant_features_constrained\"], perf_max_constraint[\"anticlassifier_score\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case it appears that the naive bayes classifier's precision was so low that the anti-classifier was always able to generate a vector that was accepted. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Support Vector Machine\n",
      "\n",
      "Finally we target a support vector machine. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm = classifier.svm()\n",
      "svm.fit(xtrain, ytrain)\n",
      "print(svm.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "perf_max_constraint = evaluate.evaluate(svm, constrain=\"max\")\n",
      "perf_max_constraint.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(perf_max_constraint[\"significant_features_constrained\"], perf_max_constraint[\"anticlassifier_score\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, the three most significant features determine how well the anti-classifier performs. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "## Conclusions\n",
      "\n",
      "We've found that a small number of features are overwhelmingly significant in determining whether or not we can \"beat\" a classifier. If the classifier is properly trained then we must know which features are most significant if we are to have any hope of beating it. \n",
      "\n",
      "Furthermore, if we are forced to constrain ourselves to a sub-space of features that the classifier is trained to detect then we also have little hope to beat it. \n",
      "\n",
      "This behaviour is consistent with a pattern seen elsewhere in machine learning and data science. Most problem spaces have only a few features which dominate the output behaviour of classification algorithms. Finding and exploiting these features is the most critical part of building a classification pipeline. \n",
      "\n",
      "This limits the usefulness of the anti-classifier approach since we must know before building the anti-classifier what the most significant features are. \n",
      "\n",
      "Nevertheless this was an interesting exercise and a good way of learning about how important feature selection is. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}