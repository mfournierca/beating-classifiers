{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Beating Classifiers\n",
      "\n",
      "## Note\n",
      "\n",
      "I am not a spammer. I've studied machine learning and classification over the last year and became interested in the topic. I've always thought that a good way to learn about how something works is to break it. The desire to break classification systems in order to learn about them is what drove me to write this document. \n",
      "\n",
      "## Introduction\n",
      "\n",
      "Imagine we are sending messages to a classifier for which we can only observe the input features and the classification result. For example: \n",
      "\n",
      "- Sending messages to an email inbox where they are filtered by a spam classifier. \n",
      "- Asking bots to scrape a website where they are filtered by a bot detection algorithm. \n",
      "\n",
      "In these cases we can have access to the input feature vectors and can find some way of getting the classification result.\n",
      "\n",
      "Using this information, we want to devise an method that can beat the classifier. \"Beating the classifier\" means crafting feature vectors which:\n",
      "\n",
      "- Satisify some requirements we define, and \n",
      "- Are not rejected by the classifier\n",
      "\n",
      "## Preliminaries\n",
      "\n",
      "### Prerequisites\n",
      "\n",
      "- Familiarity with machine learning techniques and terminology, e.g. \"classifier\", \"training set\", etc. \n",
      "- Familiarity with the Logistical Regresssion Classifier specifically, the sigmoid function, log-loss scoring, etc. \n",
      "\n",
      "### Definitions\n",
      "\n",
      "- The Classifier: The classifier we are trying to beat\n",
      "- The Anti-Classifier: The algorithm we write to try and beat the classifier\n",
      "- Message: The raw message / object that we is constructed by the anti-classifier and sent to the classifier\n",
      "- Feature Vector: A vector of features extracted from the message\n",
      "- Feature Specificiations: The data type, max / min value and name of each variable entry in the feature vector\n",
      "- Feature Contraints: A set of requirements we want our feature vector to satisfy while still passing through the classifier\n",
      "\n",
      "## General Approach\n",
      "\n",
      "We proceed as follows:\n",
      "\n",
      "- Stage One: Train the Anti-Classifier\n",
      "\n",
      "    1. Define feature specifications\n",
      "    1. Randomly generate a set of intial vectors that conform to the feature specifications\n",
      "    1. Create a logisitical regression classifier to use as our anti-classifier. \n",
      "    1. Send the randomly generated set of feature vectors to the classifier and record the result for each one\n",
      "    1. Use the results to train the anti-classifier\n",
      "\n",
      "- Stage Two: Use the Anti-Classifier\n",
      "    \n",
      "    1. Create feature constraints\n",
      "    1. Minimize the decision function of the anti-classifier under those contraints\n",
      "    1. Output the result\n",
      "\n",
      "The result can now be used to build a message to send to the classifier\n",
      "\n",
      "### Simplifications\n",
      "\n",
      "One of the most complex and error-prone portions of this approach would be a translation layer between feature vectors and messages.\n",
      "\n",
      "- Specific to each use case\n",
      "- Potential to miss information that is significant to the classifier\n",
      "\n",
      "Would the `feature -> message` translation layer in the anti-classifier need to match the `message -> feature` extraction in the classifier?\n",
      "\n",
      "This part I'm not sure about.\n",
      "\n",
      "`anti-feature space -> message -> feature space`\n",
      "\n",
      "We know nothing about the second transform. We can't gaurantee anything about the composite transform. We might be working on a different basis, a different number of dimensions, etc, from the classifier feature space. We may not even be able to influence many of the features that the classifier depends on. \n",
      "\n",
      "If we need hundreds of features for our anti-classifier then we'll need to write a translation layer, by hand, that accounts for those hundreds of features. On the other hand, in most applications classifiers depend on a small number of features that contribute most strongly to the classification result, so even if the classifier uses hundreds we may only need to consider a handful of them. \n",
      "\n",
      "If we keep looking for an effective set of classifier features we will find them eventually. It may not always be possible, or it may take a long time, but it is possible in principle. \n",
      "\n",
      "For the purposes of this document we ignore the translation layer and concentrate on the core algorithm. We give the anti-classifier access to the feature specifications of the classifier. We do not give access to the classifier parameters or the classifier training set. \n",
      "\n",
      "## Algorithm\n",
      "\n",
      "Write it down, talk about it. \n",
      "\n",
      "## Performance\n",
      "\n",
      "### Time / Computational Complexity\n",
      "\n",
      "### Performance Against a Logistical Regression Classifier\n",
      "\n",
      "#### Naive Case\n",
      "\n",
      "#### Most Significant Predictors\n",
      "\n",
      "#### Least Significant Predictors\n",
      "\n",
      "### Performance Against a Naive Bayes Classifier\n",
      "\n",
      "### Performance Against a Support Vector Machine\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}